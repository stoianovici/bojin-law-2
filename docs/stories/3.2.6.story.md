# Story 3.2.6: AI Training Pipeline for Legacy Document Processing

**Epic:** 3 - AI-Powered Document Management & Semantic Version Control
**Story Type:** Feature
**Priority:** Medium
**Estimated Effort:** 5-7 days
**Dependencies:** Story 3.1 (AI Service Infrastructure), Story 3.2.5 (Legacy Document Import & Categorization)

## User Story

**As an** AI service,
**I want** to automatically process categorized legacy documents from OneDrive,
**so that** I can learn firm-specific language patterns, clause structures, and templates to provide better document generation suggestions.

## Business Context

After partners categorize legacy documents in Story 3.2.5, those documents sit in OneDrive organized by category (Contract, Notificare Avocateasca, Intampinare, etc.). This story implements the backend pipeline that:
- Discovers new documents in OneDrive training folders
- Extracts text content from PDFs and Word documents
- Generates embeddings for semantic search
- Identifies common patterns and templates
- Stores learned knowledge for AI to reference during document generation

**Key Goal:** Enable AI to suggest firm-specific language when lawyers create new documents, making suggestions feel authentic to the firm's style.

## Acceptance Criteria

### 1. OneDrive Document Discovery

**Given** categorized documents exist in `/AI-Training/` folders in OneDrive
**When** the training pipeline runs
**Then** it should:
- ✅ Scan all category folders under `/AI-Training/`
- ✅ Read `_metadata.json` files to get document metadata
- ✅ Identify new documents not yet processed (compare against processed documents database)
- ✅ Handle multiple category folders dynamically
- ✅ Skip documents already processed (idempotent operation)
- ✅ Log discovery results (X new documents found in Y categories)

### 2. Document Text Extraction

**Given** a PDF or DOCX document needs processing
**When** the extraction process runs
**Then** it should:
- ✅ Download document from OneDrive to temporary storage
- ✅ Extract text from PDF files (including OCR for scanned documents if needed)
- ✅ Extract text from DOCX/DOC files preserving structure
- ✅ Clean extracted text (remove extra whitespace, normalize encoding)
- ✅ Detect document language (Romanian vs English)
- ✅ Handle extraction errors gracefully (log and skip, don't fail pipeline)
- ✅ Delete temporary files after processing

### 3. Embedding Generation

**Given** text has been extracted from a document
**When** generating embeddings
**Then** the system should:
- ✅ Split large documents into chunks (max 512 tokens per chunk)
- ✅ Generate embeddings using OpenAI text-embedding-3-small model
- ✅ Store embeddings in PostgreSQL with pgvector extension
- ✅ Associate embeddings with document metadata (category, original filename, folder path)
- ✅ Track token usage for embedding generation
- ✅ Batch process embeddings (10 documents at a time) to optimize API calls

### 4. Pattern & Template Identification

**Given** multiple documents in the same category
**When** analyzing for patterns
**Then** the system should:
- ✅ Identify repeated phrases across documents (min 5 words, appears in 3+ docs)
- ✅ Extract common document structures (heading patterns, section ordering)
- ✅ Detect standard clauses by category (e.g., liability clauses in Contracts)
- ✅ Store patterns in `DocumentPatterns` table with usage count
- ✅ Calculate similarity scores between documents in same category
- ✅ Flag high-similarity documents (>85%) as template candidates

### 5. Knowledge Base Storage

**Given** processed documents and identified patterns
**When** storing in the knowledge base
**Then** the system should:
- ✅ Store in `TrainingDocuments` table: documentId, category, textContent, metadata, processedAt
- ✅ Store in `DocumentEmbeddings` table: embeddingId, documentId, chunkText, embedding (vector), chunkIndex
- ✅ Store in `DocumentPatterns` table: patternId, category, patternText, frequency, documentIds[]
- ✅ Store in `TemplateLibrary` table: templateId, category, baseDocumentId, structure, usageCount
- ✅ Create indexes for fast semantic search queries
- ✅ Enable full-text search on textContent using PostgreSQL tsvector

### 6. Pipeline Orchestration

**Given** the training pipeline needs to run
**When** triggered (scheduled or manual)
**Then** it should:
- ✅ Run as a background job (BullMQ queue)
- ✅ Process documents in batches (10 at a time)
- ✅ Track progress in `TrainingPipelineRuns` table
- ✅ Support manual trigger via admin API endpoint
- ✅ Run automatically on a schedule (daily at 2 AM)
- ✅ Send notification on completion with stats (X docs processed, Y patterns found)
- ✅ Handle failures gracefully (retry individual documents up to 3 times)

### 7. AI Integration for Document Generation

**Given** the knowledge base has processed legacy documents
**When** a user creates a new document in the app
**Then** the AI service should:
- ✅ Accept document type/category as context
- ✅ Query similar documents via semantic search (top 5 matches)
- ✅ Retrieve relevant patterns and templates for the category
- ✅ Include firm-specific clauses in generation prompt
- ✅ Suggest template structures from similar historical documents
- ✅ This story provides the data; AI prompt engineering is out of scope

## Technical Approach

### Architecture Overview

```
┌─────────────────────────────────────────────────────────┐
│                   OneDrive Storage                      │
│              /AI-Training/{CategoryName}/               │
└────────────────────────┬────────────────────────────────┘
                         │
                         ▼
┌─────────────────────────────────────────────────────────┐
│              Training Pipeline (BullMQ Job)             │
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐  │
│  │   Discovery  │→│  Extraction  │→│  Embedding   │  │
│  │   Service    │  │   Service    │  │  Generation  │  │
│  └──────────────┘  └──────────────┘  └──────────────┘  │
│           │               │                  │          │
│           ▼               ▼                  ▼          │
│  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐  │
│  │   Pattern    │  │   Template   │  │   Knowledge  │  │
│  │   Analysis   │  │  Extraction  │  │     Base     │  │
│  └──────────────┘  └──────────────┘  └──────────────┘  │
└─────────────────────────────────────────────────────────┘
                         │
                         ▼
┌─────────────────────────────────────────────────────────┐
│         PostgreSQL + pgvector Knowledge Base            │
│  ┌──────────────────────────────────────────────────┐   │
│  │ TrainingDocuments | DocumentEmbeddings           │   │
│  │ DocumentPatterns  | TemplateLibrary              │   │
│  │ TrainingPipelineRuns                             │   │
│  └──────────────────────────────────────────────────┘   │
└─────────────────────────────────────────────────────────┘
                         │
                         ▼
┌─────────────────────────────────────────────────────────┐
│              AI Service (Story 3.1)                     │
│        Uses knowledge base for document generation      │
└─────────────────────────────────────────────────────────┘
```

### Technology Stack

| Component | Technology | Rationale |
|-----------|-----------|-----------|
| **Job Queue** | BullMQ + Redis | Reliable background job processing, retry support |
| **PDF Extraction** | pdf-parse npm package | Lightweight, handles most PDFs well |
| **OCR (if needed)** | Tesseract.js | Client-side OCR for scanned PDFs |
| **DOCX Extraction** | mammoth.js | Clean text extraction from Word docs |
| **Embeddings** | OpenAI text-embedding-3-small | Cost-effective, 1536 dimensions, multilingual |
| **Vector DB** | PostgreSQL pgvector | Already in stack, no new dependencies |
| **Text Processing** | natural npm package | Tokenization, pattern matching |
| **OneDrive Access** | Microsoft Graph API | Existing integration from Story 2.9 |
| **Scheduling** | node-cron | Simple cron-like scheduling |

### Database Schema

```sql
-- Training Documents Table
CREATE TABLE training_documents (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    category VARCHAR(255) NOT NULL,
    original_filename VARCHAR(500) NOT NULL,
    original_folder_path TEXT,
    one_drive_file_id VARCHAR(255) UNIQUE NOT NULL,
    text_content TEXT NOT NULL,
    language VARCHAR(10) NOT NULL, -- 'ro' or 'en'
    word_count INTEGER,
    metadata JSONB, -- email subject, sender, date, etc.
    processed_at TIMESTAMPTZ NOT NULL DEFAULT CURRENT_TIMESTAMP,
    processing_duration_ms INTEGER,
    created_at TIMESTAMPTZ NOT NULL DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_training_docs_category ON training_documents(category);
CREATE INDEX idx_training_docs_onedrive ON training_documents(one_drive_file_id);
CREATE INDEX idx_training_docs_processed ON training_documents(processed_at);

-- Document Embeddings Table
CREATE TABLE document_embeddings (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    document_id UUID NOT NULL REFERENCES training_documents(id) ON DELETE CASCADE,
    chunk_index INTEGER NOT NULL,
    chunk_text TEXT NOT NULL,
    embedding vector(1536) NOT NULL, -- pgvector type
    token_count INTEGER,
    created_at TIMESTAMPTZ NOT NULL DEFAULT CURRENT_TIMESTAMP,
    UNIQUE(document_id, chunk_index)
);

CREATE INDEX idx_embeddings_document ON document_embeddings(document_id);
CREATE INDEX idx_embeddings_vector ON document_embeddings USING ivfflat (embedding vector_cosine_ops);

-- Document Patterns Table
CREATE TABLE document_patterns (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    category VARCHAR(255) NOT NULL,
    pattern_type VARCHAR(50) NOT NULL, -- 'phrase', 'clause', 'structure'
    pattern_text TEXT NOT NULL,
    frequency INTEGER DEFAULT 1,
    document_ids UUID[] NOT NULL, -- Array of document IDs containing this pattern
    confidence_score DECIMAL(3,2), -- 0.00 to 1.00
    metadata JSONB,
    created_at TIMESTAMPTZ NOT NULL DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMPTZ NOT NULL DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_patterns_category ON document_patterns(category);
CREATE INDEX idx_patterns_type ON document_patterns(pattern_type);
CREATE INDEX idx_patterns_frequency ON document_patterns(frequency DESC);

-- Template Library Table
CREATE TABLE template_library (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    category VARCHAR(255) NOT NULL,
    name VARCHAR(500),
    base_document_id UUID REFERENCES training_documents(id),
    structure JSONB NOT NULL, -- Sections, headings, clause order
    similar_document_ids UUID[], -- Documents used to build this template
    usage_count INTEGER DEFAULT 0,
    quality_score DECIMAL(3,2), -- AI-assessed quality 0.00 to 1.00
    created_at TIMESTAMPTZ NOT NULL DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMPTZ NOT NULL DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_templates_category ON template_library(category);
CREATE INDEX idx_templates_usage ON template_library(usage_count DESC);
CREATE INDEX idx_templates_quality ON template_library(quality_score DESC);

-- Training Pipeline Runs Table
CREATE TABLE training_pipeline_runs (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    run_type VARCHAR(50) NOT NULL, -- 'scheduled', 'manual'
    status VARCHAR(50) NOT NULL, -- 'running', 'completed', 'failed'
    started_at TIMESTAMPTZ NOT NULL DEFAULT CURRENT_TIMESTAMP,
    completed_at TIMESTAMPTZ,
    documents_discovered INTEGER DEFAULT 0,
    documents_processed INTEGER DEFAULT 0,
    documents_failed INTEGER DEFAULT 0,
    patterns_identified INTEGER DEFAULT 0,
    templates_created INTEGER DEFAULT 0,
    total_tokens_used INTEGER DEFAULT 0,
    error_log JSONB,
    metadata JSONB,
    created_at TIMESTAMPTZ NOT NULL DEFAULT CURRENT_TIMESTAMP
);

CREATE INDEX idx_pipeline_runs_status ON training_pipeline_runs(status);
CREATE INDEX idx_pipeline_runs_started ON training_pipeline_runs(started_at DESC);
```

### Data Models (TypeScript)

```typescript
export interface TrainingDocument {
  id: string;
  category: string;
  originalFilename: string;
  originalFolderPath?: string;
  oneDriveFileId: string;
  textContent: string;
  language: 'ro' | 'en';
  wordCount: number;
  metadata?: {
    emailSubject?: string;
    emailSender?: string;
    emailDate?: Date;
    fileSize?: number;
  };
  processedAt: Date;
  processingDurationMs: number;
}

export interface DocumentEmbedding {
  id: string;
  documentId: string;
  chunkIndex: number;
  chunkText: string;
  embedding: number[]; // 1536-dimensional vector
  tokenCount: number;
}

export interface DocumentPattern {
  id: string;
  category: string;
  patternType: 'phrase' | 'clause' | 'structure';
  patternText: string;
  frequency: number;
  documentIds: string[];
  confidenceScore: number;
  metadata?: Record<string, any>;
}

export interface TemplateStructure {
  sections: {
    heading: string;
    order: number;
    commonPhrases: string[];
  }[];
  totalSections: number;
  avgSectionLength: number;
}

export interface DocumentTemplate {
  id: string;
  category: string;
  name: string;
  baseDocumentId: string;
  structure: TemplateStructure;
  similarDocumentIds: string[];
  usageCount: number;
  qualityScore: number;
}

export interface TrainingPipelineRun {
  id: string;
  runType: 'scheduled' | 'manual';
  status: 'running' | 'completed' | 'failed';
  startedAt: Date;
  completedAt?: Date;
  documentsDiscovered: number;
  documentsProcessed: number;
  documentsFailed: number;
  patternsIdentified: number;
  templatesCreated: number;
  totalTokensUsed: number;
  errorLog?: Record<string, any>;
}
```

### Processing Pipeline Flow

```typescript
// Simplified pseudocode for the pipeline

async function runTrainingPipeline(runType: 'scheduled' | 'manual') {
  const run = await createPipelineRun(runType);

  try {
    // Phase 1: Discovery
    const categories = await discoverOneDriveFolders('/AI-Training/');
    const newDocuments = await findUnprocessedDocuments(categories);
    run.documentsDiscovered = newDocuments.length;

    // Phase 2: Process in batches
    const batches = chunk(newDocuments, 10);

    for (const batch of batches) {
      await Promise.all(batch.map(async (doc) => {
        try {
          // Extract text
          const text = await extractText(doc);

          // Generate embeddings
          const chunks = splitIntoChunks(text, 512);
          const embeddings = await generateEmbeddings(chunks);

          // Store
          await storeTrainingDocument(doc, text, embeddings);
          run.documentsProcessed++;

        } catch (error) {
          run.documentsFailed++;
          logError(doc, error);
        }
      }));
    }

    // Phase 3: Pattern Analysis (runs on all docs in category)
    for (const category of categories) {
      const patterns = await identifyPatterns(category);
      run.patternsIdentified += patterns.length;

      const templates = await extractTemplates(category);
      run.templatesCreated += templates.length;
    }

    run.status = 'completed';
    await notifyCompletion(run);

  } catch (error) {
    run.status = 'failed';
    run.errorLog = { error: error.message };
  } finally {
    run.completedAt = new Date();
    await savePipelineRun(run);
  }
}
```

## Implementation Plan

### Phase 1: OneDrive Integration & Discovery (Days 1-2)

1. Create discovery service to scan `/AI-Training/` folders
2. Read `_metadata.json` files from each category
3. Compare with `training_documents` table to find new docs
4. Implement download from OneDrive to temp storage
5. Unit tests for discovery logic

### Phase 2: Text Extraction (Days 2-3)

1. Integrate pdf-parse for PDF text extraction
2. Integrate mammoth.js for DOCX extraction
3. Implement text cleaning and normalization
4. Language detection (Romanian vs English)
5. Error handling for corrupted files
6. Unit tests for extraction logic

### Phase 3: Embedding Generation & Storage (Days 3-4)

1. Implement text chunking (max 512 tokens)
2. Integrate OpenAI embedding API
3. Store embeddings in PostgreSQL with pgvector
4. Create database tables and indexes
5. Batch processing optimization
6. Unit tests for embedding generation

### Phase 4: Pattern & Template Analysis (Days 4-5)

1. Implement pattern detection algorithm
2. Identify common phrases across documents
3. Extract document structure patterns
4. Create template candidates from high-similarity docs
5. Store patterns and templates in database
6. Unit tests for pattern analysis

### Phase 5: Pipeline Orchestration (Days 5-6)

1. Set up BullMQ job queue
2. Create pipeline orchestration job
3. Implement batch processing
4. Add retry logic for failures
5. Create admin API endpoint for manual trigger
6. Set up cron schedule (daily at 2 AM)

### Phase 6: Testing & Monitoring (Days 6-7)

1. Integration tests with real OneDrive data
2. Performance testing with 100+ documents
3. Error handling and edge cases
4. Monitoring dashboard for pipeline runs
5. Documentation for operations team

## Testing Strategy

### Unit Tests
- OneDrive folder discovery
- Text extraction from PDF and DOCX
- Text chunking algorithm
- Pattern detection logic
- Template structure extraction
- Database operations (CRUD)

### Integration Tests
- End-to-end pipeline with sample documents
- OneDrive download and processing
- Embedding generation and storage
- Pattern analysis across multiple documents
- Error handling and retry logic

### Performance Tests
- Process 100 documents benchmark
- Memory usage during batch processing
- Database query performance for semantic search
- Embedding generation throughput

### Manual Testing Checklist
- [ ] Upload 10 sample documents to OneDrive `/AI-Training/Contract/`
- [ ] Trigger pipeline manually via API
- [ ] Verify all 10 documents processed successfully
- [ ] Query semantic search for similar documents
- [ ] Verify patterns identified correctly
- [ ] Check template library for new templates
- [ ] Test with Romanian and English documents
- [ ] Verify error handling with corrupted PDF

## Rollback Plan

- Pipeline runs as background job - no impact on main app if issues arise
- Feature flag to disable pipeline completely
- Can reprocess documents by deleting from `training_documents` table
- Database migrations have rollback scripts
- Failed runs logged for debugging without blocking future runs

## Definition of Done

- [ ] All acceptance criteria met
- [ ] Unit tests pass with >80% coverage
- [ ] Integration tests pass
- [ ] Performance benchmarks met (100 docs in <10 minutes)
- [ ] Code reviewed and approved
- [ ] Database schema migrated to production
- [ ] Monitoring dashboard deployed
- [ ] Pipeline runs successfully on schedule
- [ ] Manual trigger endpoint tested
- [ ] Documentation created for operations

## Success Metrics

- Successfully processes >95% of discovered documents
- Embedding generation completes in <5 seconds per document
- Pattern detection identifies >10 common patterns per category
- Template library created with >3 templates per category
- Pipeline completes daily run in <30 minutes for 100 documents
- Zero pipeline failures in first week of operation
- Token costs under €5 per 100 documents processed

## API Endpoints (New)

### Manual Pipeline Trigger
```
POST /api/admin/training-pipeline/trigger
Authorization: Bearer {admin_token}

Response:
{
  "runId": "uuid",
  "status": "running",
  "startedAt": "2024-11-13T10:00:00Z"
}
```

### Pipeline Status
```
GET /api/admin/training-pipeline/runs/{runId}

Response:
{
  "id": "uuid",
  "status": "completed",
  "documentsProcessed": 47,
  "patternsIdentified": 23,
  "templatesCreated": 5,
  "completedAt": "2024-11-13T10:15:00Z"
}
```

### Semantic Search (for AI Service)
```
POST /api/ai/knowledge-base/search
Content-Type: application/json

{
  "query": "liability clause in contract",
  "category": "Contract",
  "limit": 5
}

Response:
{
  "results": [
    {
      "documentId": "uuid",
      "chunkText": "...",
      "similarity": 0.89,
      "metadata": { ... }
    }
  ]
}
```

## Integration with Existing Stories

### Story 3.1 (AI Service Infrastructure)
- AI service will call semantic search endpoint
- Use knowledge base for document generation prompts
- Track token usage for training pipeline

### Story 3.2 (Document Template Learning - Original PRD)
- This story COMPLETES Story 3.2's vision
- Original Story 3.2 mentioned "bulk import via Outlook mailbox attachment scanning"
- Stories 3.2.5 + 3.2.6 together deliver full template learning capability

### Story 3.3 (Intelligent Document Drafting)
- Will query template library for relevant templates
- Use patterns for clause suggestions
- Leverage semantic search to find similar historical documents

## Future Enhancements (Out of Scope)

- Real-time incremental training (vs batch daily)
- Multi-language embedding support beyond English/Romanian
- Advanced OCR for handwritten documents
- Automatic template quality scoring refinement
- Pattern recommendation engine for new document types
- A/B testing different embedding models
- Cost optimization via embedding caching

---

**Story Status:** Ready for Implementation
**Created:** 2024-11-13
**Last Updated:** 2024-11-13
**Author:** Mary (Business Analyst)
