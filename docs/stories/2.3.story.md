# Story 2.3: Data Migration and Seeding Strategy

## Status
**Draft**

## Story

**As a** development team,
**I want** data migration and seeding capabilities,
**so that** we can manage schema changes and test data effectively.

## Acceptance Criteria

1. Prisma migrations configured with history tracking and rollback capabilities
2. Seed data script creates test law firm, sample cases, documents, tasks
3. Data anonymization script for production data in dev
4. Backup and restore procedures documented
5. Migration runbook for production deployments
6. Zero-downtime migration strategy defined

## Tasks / Subtasks

### Phase 1: Prisma Migration Infrastructure (AC: 1)

- [ ] **Task 1: Configure Prisma Migrations** (AC: 1)
  - [ ] Verify Prisma generator and datasource configuration in schema.prisma
  - [ ] Initialize Prisma migrations directory if not exists: `npx prisma migrate dev --name init`
  - [ ] Configure migration history tracking in database (automatic via Prisma)
  - [ ] Create migration script wrapper: `packages/database/scripts/run-migration.sh`
  - [ ] Add npm scripts for migration commands: `db:migrate`, `db:migrate:status`, `db:migrate:undo`
  - [ ] Test migration creation with example schema change
  - [ ] Document Prisma migration workflow in packages/database/README.md

- [ ] **Task 2: Implement Migration Rollback Capability** (AC: 1)
  - [ ] Create rollback script: `packages/database/scripts/rollback-migration.sh`
  - [ ] Implement migration state verification before rollback
  - [ ] Add safety checks: confirm prompt, backup reminder, production environment warning
  - [ ] Test rollback on development database
  - [ ] Document rollback procedure with examples
  - [ ] Add rollback command to npm scripts: `db:migrate:rollback`

- [ ] **Task 3: Create Migration History Tracking** (AC: 1)
  - [ ] Verify Prisma `_prisma_migrations` table tracks history automatically
  - [ ] Create utility script to view migration history: `packages/database/scripts/migration-history.sh`
  - [ ] Add migration metadata fields (description, author, timestamp) via comments
  - [ ] Document how to query migration history from database
  - [ ] Add npm script: `db:migrate:history`

### Phase 2: Seed Data Implementation (AC: 2)

- [ ] **Task 4: Design Seed Data Structure** (AC: 2)
  - [ ] Define test law firm profile (name, address, contact info)
  - [ ] Define sample users: 1 Partner, 2 Associates, 2 Paralegals
  - [ ] Define 10 sample cases covering different case types and statuses
  - [ ] Define 20 sample documents with various types and statuses
  - [ ] Define 30 sample tasks covering all task types
  - [ ] Create seed data schema documentation
  - [ ] Ensure seed data includes edge cases for testing

- [ ] **Task 5: Implement Seed Data Script** (AC: 2)
  - [ ] Create seed script: `packages/database/prisma/seed.ts`
  - [ ] Implement law firm creation with idempotency (check if exists)
  - [ ] Implement user creation with Azure AD mock IDs
  - [ ] Implement case creation with proper relationships
  - [ ] Implement document creation with mock storage URLs
  - [ ] Implement task creation with proper assignments
  - [ ] Add UUID generation for all entities
  - [ ] Handle foreign key relationships correctly
  - [ ] Add transaction wrapper for atomic seed operations
  - [ ] Configure Prisma seed command in package.json

- [ ] **Task 6: Test Seed Data Script** (AC: 2)
  - [ ] Run seed script on clean development database
  - [ ] Verify all entities created with correct counts
  - [ ] Verify relationships maintained (foreign keys valid)
  - [ ] Verify data integrity constraints satisfied
  - [ ] Test idempotency: run seed script twice, verify no duplicates
  - [ ] Test seed script with existing data
  - [ ] Add seed script to integration tests
  - [ ] Document seed data usage and limitations

### Phase 3: Data Anonymization (AC: 3)

- [ ] **Task 7: Implement Data Anonymization Script** (AC: 3)
  - [ ] Create anonymization script: `packages/database/scripts/anonymize-data.ts`
  - [ ] Anonymize user PII: names, emails (keep domain structure)
  - [ ] Anonymize client information: names, addresses, contact info
  - [ ] Anonymize case details: replace with generic descriptions
  - [ ] Anonymize document content: replace with lorem ipsum
  - [ ] Preserve data structure and relationships
  - [ ] Preserve statistical distributions (case types, statuses)
  - [ ] Add configuration for which fields to anonymize
  - [ ] Implement as database transaction for rollback capability

- [ ] **Task 8: Create Production Data Export Utility** (AC: 3)
  - [ ] Create export script: `packages/database/scripts/export-production.sh`
  - [ ] Export production database to SQL file with pg_dump
  - [ ] Add export timestamp and metadata
  - [ ] Compress exported file (gzip)
  - [ ] Store export in secure location (not in git)
  - [ ] Add npm script: `db:export:production`
  - [ ] Document export process and security considerations

- [ ] **Task 9: Create Anonymized Import Workflow** (AC: 3)
  - [ ] Create import script: `packages/database/scripts/import-anonymized.sh`
  - [ ] Import production export to development database
  - [ ] Run anonymization script automatically after import
  - [ ] Verify anonymization completed successfully
  - [ ] Add data validation checks post-anonymization
  - [ ] Document complete workflow: export → anonymize → import
  - [ ] Add npm script: `db:import:anonymized`

### Phase 4: Backup and Restore Procedures (AC: 4)

- [ ] **Task 10: Document Automated Backup Strategy** (AC: 4)
  - [ ] Document Render automatic daily backups (7-day retention)
  - [ ] Document backup schedule and retention policy
  - [ ] Document how to view backup status in Render Dashboard
  - [ ] Document backup storage location and access
  - [ ] Add backup verification checklist
  - [ ] Update infrastructure/OPERATIONS_RUNBOOK.md with backup procedures

- [ ] **Task 11: Create Manual Backup Procedures** (AC: 4)
  - [ ] Create manual backup script: `packages/database/scripts/backup-database.sh`
  - [ ] Use Render CLI: `render db backup --database [db-name]`
  - [ ] Use pg_dump for local backups: `pg_dump $DATABASE_URL > backup.sql`
  - [ ] Add backup naming convention: `backup-{env}-{date}-{time}.sql`
  - [ ] Store backups securely (S3, Cloudflare R2, or local encrypted)
  - [ ] Document when to trigger manual backups (before migrations, major changes)
  - [ ] Add npm script: `db:backup`

- [ ] **Task 12: Create Database Restore Procedures** (AC: 4)
  - [ ] Create restore script: `packages/database/scripts/restore-database.sh`
  - [ ] Implement Render restore: `render db restore --database [db-name] --backup [backup-id]`
  - [ ] Implement pg_restore for local restores
  - [ ] Add safety checks: confirm prompt, environment verification
  - [ ] Add pre-restore backup of current database (backup-before-restore)
  - [ ] Document restore process with step-by-step instructions
  - [ ] Test restore on staging environment
  - [ ] Document rollback if restore fails
  - [ ] Add npm script: `db:restore`

- [ ] **Task 13: Test Backup and Restore End-to-End** (AC: 4)
  - [ ] Create test database with sample data
  - [ ] Perform manual backup
  - [ ] Destroy test database
  - [ ] Restore from backup
  - [ ] Verify all data restored correctly
  - [ ] Verify database integrity post-restore
  - [ ] Document test results and any issues encountered
  - [ ] Add backup/restore test to integration test suite

### Phase 5: Migration Runbook (AC: 5)

- [ ] **Task 14: Create Production Migration Runbook** (AC: 5)
  - [ ] Create runbook: `docs/runbooks/database-migration-runbook.md`
  - [ ] Document pre-migration checklist (backup, team notification, rollback plan)
  - [ ] Document migration execution steps (staging → production)
  - [ ] Document migration verification steps (data integrity checks)
  - [ ] Document post-migration monitoring (error rates, performance)
  - [ ] Document rollback procedure (when to rollback, how to rollback)
  - [ ] Include example migration timeline with communication plan
  - [ ] Add troubleshooting section for common migration issues

- [ ] **Task 15: Define Migration Testing Strategy** (AC: 5)
  - [ ] Test migrations on local development database first
  - [ ] Test migrations on staging environment second
  - [ ] Verify schema changes match expectations (use `prisma migrate diff`)
  - [ ] Run full test suite after migration (unit, integration, E2E)
  - [ ] Perform manual smoke tests on critical workflows
  - [ ] Measure migration performance (time to complete)
  - [ ] Document testing checklist in migration runbook

- [ ] **Task 16: Create Migration Communication Template** (AC: 5)
  - [ ] Create template: `docs/templates/migration-announcement-template.md`
  - [ ] Include fields: migration purpose, downtime estimate, affected features
  - [ ] Include timeline: start time, expected completion, rollback deadline
  - [ ] Include contact information for issues
  - [ ] Include rollback criteria
  - [ ] Document when to send announcements (24h before, 1h before, during, after)

### Phase 6: Zero-Downtime Migration Strategy (AC: 6)

- [ ] **Task 17: Design Zero-Downtime Migration Patterns** (AC: 6)
  - [ ] Document expand-contract pattern for schema changes
  - [ ] Document backward-compatible migration strategy
  - [ ] Document blue-green deployment for breaking changes
  - [ ] Document feature flags for gradual schema rollout
  - [ ] Identify migration types: additive (safe), destructive (requires downtime)
  - [ ] Create decision matrix for migration strategy selection
  - [ ] Document in docs/architecture/database-migration-patterns.md

- [ ] **Task 18: Implement Expand-Contract Migration Example** (AC: 6)
  - [ ] Create example: renaming a column using expand-contract
  - [ ] Step 1: Add new column (expand)
  - [ ] Step 2: Dual-write to both columns
  - [ ] Step 3: Backfill data to new column
  - [ ] Step 4: Switch reads to new column
  - [ ] Step 5: Remove old column (contract)
  - [ ] Document each step with Prisma migration code examples
  - [ ] Test example on staging environment

- [ ] **Task 19: Create Migration Risk Assessment Checklist** (AC: 6)
  - [ ] Create checklist: `docs/runbooks/migration-risk-assessment.md`
  - [ ] Assess if migration is backward-compatible
  - [ ] Assess if migration requires downtime
  - [ ] Assess data volume impact (large table migrations)
  - [ ] Assess foreign key constraints and cascading effects
  - [ ] Assess index creation time (can lock tables)
  - [ ] Define risk levels: low (additive), medium (backfill), high (destructive)
  - [ ] Document mitigation strategies for each risk level

### Phase 7: Integration Testing and Documentation (AC: All)

- [ ] **Task 20: Create Migration Integration Tests**
  - [ ] Create test: `tests/integration/migrations.test.ts`
  - [ ] Test: Create migration from schema change
  - [ ] Test: Apply migration to test database
  - [ ] Test: Rollback migration
  - [ ] Test: Migration history tracking
  - [ ] Test: Seed data script execution
  - [ ] Test: Data anonymization correctness
  - [ ] Test: Backup and restore integrity
  - [ ] Achieve 80%+ coverage on migration utilities

- [ ] **Task 21: Update packages/database/README.md**
  - [ ] Add "Migrations" section with usage guide
  - [ ] Add "Seed Data" section with examples
  - [ ] Add "Backup and Restore" section with procedures
  - [ ] Add "Data Anonymization" section with usage
  - [ ] Add "Troubleshooting" section for common issues
  - [ ] Add command reference table (all npm scripts)
  - [ ] Add FAQ section

- [ ] **Task 22: Create Developer Quick Start Guide**
  - [ ] Create guide: `docs/runbooks/database-quick-start.md`
  - [ ] Document: Setting up local database
  - [ ] Document: Running migrations
  - [ ] Document: Seeding test data
  - [ ] Document: Resetting database
  - [ ] Document: Common workflows (add table, add column, rename field)
  - [ ] Add troubleshooting section

## Dev Notes

### Context: Database Schema Management Foundation

This story establishes the database migration, seeding, and backup infrastructure required before implementing the full database schema in Story 2.4 (Authentication) and Story 2.6 (Case Management). It builds directly on Story 2.2's database infrastructure setup (PostgreSQL with extensions) by adding the operational tooling needed to safely evolve the schema and manage data throughout the development and production lifecycle.

**Key Dependencies:**
- Story 2.2 (Cloud Infrastructure and Database Setup) must be complete
- PostgreSQL database with extensions (pgvector, uuid-ossp, pg_trgm) must be enabled
- Render database service must be provisioned

**Blocks:**
- Story 2.4: Authentication (needs migration infrastructure to add users/sessions tables)
- Story 2.6: Case Management (needs seed data for testing case workflows)
- All future schema changes require migration capability

### Previous Story Insights

From Story 2.2 (Cloud Infrastructure and Database Setup):

1. **Database Configuration:**
   - PostgreSQL Standard 25GB configured on Render
   - Extensions enabled: pgvector (0.5+), uuid-ossp, pg_trgm
   - Connection pooling: max 20 connections, pool size 10
   - Automated daily backups with 7-day retention (Render managed)

2. **Prisma Setup:**
   - Prisma Client configured at `packages/database/src/client.ts`
   - Schema location: `packages/database/prisma/schema.prisma`
   - Minimal schema exists with DatabaseHealth placeholder model
   - Extensions configured in datasource: `extensions = [vector, uuid_ossp(map: "uuid-ossp"), pg_trgm]`

3. **Migration Directory:**
   - Migrations stored at: `packages/database/migrations/`
   - Current migrations: 000_enable_extensions.sql, 001_add_skills_tables.sql, 002_add_discovery_tables.sql
   - Migration naming pattern: `{number}_{description}.sql`

4. **Backup Strategy:**
   - Render automatic daily backups (7-day retention on Standard tier)
   - Manual backup capability via Render CLI
   - Backup verification documented in OPERATIONS_RUNBOOK.md

### Prisma Migration Infrastructure

**Prisma Configuration:**
[Source: packages/database/prisma/schema.prisma, architecture/tech-stack.md]

- **ORM:** Prisma 5.8+
- **Migration Strategy:** Prisma Migrate (automatic migration generation from schema changes)
- **Generator Configuration:**
  ```prisma
  generator client {
    provider        = "prisma-client-js"
    previewFeatures = ["postgresqlExtensions"]
    output          = "../node_modules/.prisma/client"
  }
  ```
- **Datasource Configuration:**
  ```prisma
  datasource db {
    provider   = "postgresql"
    url        = env("DATABASE_URL")
    extensions = [vector, uuid_ossp(map: "uuid-ossp"), pg_trgm]
  }
  ```

**Prisma Migrate Commands:**
[Source: architecture/tech-stack.md, packages/database/README.md]

- `npx prisma migrate dev` - Create and apply migration in development
- `npx prisma migrate deploy` - Apply migrations in production (non-interactive)
- `npx prisma migrate status` - View migration history and pending migrations
- `npx prisma migrate resolve` - Mark migration as applied/rolled back manually
- `npx prisma migrate diff` - Preview schema changes before creating migration

**Migration History Tracking:**
[Source: Prisma documentation]

- Prisma automatically creates `_prisma_migrations` table
- Tracks: migration name, checksum, applied timestamp, rollback timestamp
- Migration files stored in `packages/database/prisma/migrations/`
- Each migration gets unique directory: `{timestamp}_{name}/migration.sql`

**Rollback Capability:**
[Source: infrastructure/MIGRATION_CHECKLIST.md]

- Prisma does not have automatic rollback (design decision for safety)
- Manual rollback requires:
  1. Identify migration to undo from history
  2. Write DOWN migration SQL manually
  3. Execute DOWN migration against database
  4. Mark migration as rolled back: `npx prisma migrate resolve --rolled-back {migration-name}`
- Best practice: Always test migrations on staging before production

### Seed Data Requirements

**Seed Data Structure:**
[Source: architecture/database-schema.md, Epic 2 AC 2]

**Law Firm Entity:**
- Name: "Demo Law Firm S.R.L."
- Address: "Strada Demo 123, Bucharest, Romania"
- VAT ID: "RO12345678"
- Contact: demo@lawfirm.ro, +40-123-456-789

**Sample Users (5 users):**
[Source: architecture/database-schema.md]

| Role      | Count | Email Pattern              | Azure AD ID Pattern |
|-----------|-------|----------------------------|---------------------|
| Partner   | 1     | partner@demo.lawfirm.ro    | aad-partner-demo    |
| Associate | 2     | associate1@demo.lawfirm.ro | aad-assoc1-demo     |
| Paralegal | 2     | paralegal1@demo.lawfirm.ro | aad-para1-demo      |

**Sample Cases (10 cases):**
[Source: architecture/database-schema.md]

- Case statuses: 4 Active, 2 OnHold, 2 Closed, 2 Archived
- Case types: Mix of all case_type enum values
- Each case has: case_number (unique), title, client_id (mock), description
- Opened dates range: last 2 years

**Sample Documents (20 documents):**
[Source: architecture/database-schema.md]

- Document types: Mix of all document_type enum values
- Document statuses: 8 Draft, 6 Review, 4 Approved, 2 Filed
- 50% marked as ai_generated: true
- storage_url: Mock URLs to Cloudflare R2 or local paths
- content_embedding: NULL (will be generated in Story 2.10)

**Sample Tasks (30 tasks):**
[Source: architecture/database-schema.md]

- Task types: Mix of all task_type enum values (Research, DocumentCreation, etc.)
- Assigned to different users (Paralegals, Associates)
- Due dates: range from past (overdue) to future (upcoming)
- Statuses: Pending, InProgress, Completed

**Seed Script Implementation:**
[Source: architecture/coding-standards.md]

- Location: `packages/database/prisma/seed.ts`
- Language: TypeScript
- Execution: `npx prisma db seed` (configured in package.json)
- Idempotency: Check if firm/users exist before creating (using email as unique key)
- Transaction Wrapper: Wrap entire seed in `prisma.$transaction()` for atomicity
- Error Handling: Rollback on any error, log detailed error messages

**Prisma Seed Configuration:**
[Source: Prisma documentation]

Add to `packages/database/package.json`:
```json
{
  "prisma": {
    "seed": "ts-node prisma/seed.ts"
  }
}
```

### Data Anonymization Strategy

**Anonymization Requirements:**
[Source: Epic 2 AC 3, GDPR compliance considerations]

**PII Fields to Anonymize:**

| Entity   | Fields                        | Anonymization Strategy              |
|----------|-------------------------------|-------------------------------------|
| Users    | first_name, last_name         | Replace with "Demo User {N}"        |
| Users    | email                         | Replace with demo{N}@example.com    |
| Users    | azure_ad_id                   | Replace with random UUID            |
| Clients  | name, address, contact_info   | Replace with "Demo Client {N}"      |
| Cases    | title, description            | Replace with generic descriptions   |
| Documents| title, content                | Replace with lorem ipsum            |

**Data to Preserve:**

- Database structure (tables, columns, indexes)
- Relationships (foreign keys, UUIDs maintained)
- Statistical distributions (case types, statuses, dates)
- Data volumes (same number of records)
- Enums and categorical data (unchanged)

**Anonymization Script:**
[Source: architecture/coding-standards.md]

- Location: `packages/database/scripts/anonymize-data.ts`
- Execution: `npm run db:anonymize`
- Configuration: JSON config file specifying fields to anonymize
- Transaction: Wrap entire anonymization in database transaction
- Verification: Post-anonymization check that no real PII remains
- Logging: Log number of records anonymized per table

**Production Export Workflow:**
[Source: infrastructure/OPERATIONS_RUNBOOK.md]

1. Export production database:
   ```bash
   render db backup --database bojin-law-db
   # or
   pg_dump $DATABASE_URL > export-production-$(date +%Y%m%d).sql
   ```
2. Download export to local machine (secure transfer)
3. Import to development database:
   ```bash
   psql $DATABASE_URL_DEV < export-production-20250120.sql
   ```
4. Run anonymization script:
   ```bash
   npm run db:anonymize
   ```
5. Verify anonymization completed successfully
6. Delete original export file (security)

### Backup and Restore Procedures

**Render Automated Backups:**
[Source: infrastructure/OPERATIONS_RUNBOOK.md, Story 2.2 Dev Notes]

- **Schedule:** Daily at 2:00 AM UTC
- **Retention:** 7 days (Standard tier), 14 days (Pro tier)
- **Location:** Render-managed storage (automatic)
- **Access:** Via Render Dashboard → Databases → Backups tab
- **Restoration:** `render db restore --database [db-name] --backup [backup-id]`
- **Cost:** Included in database tier pricing ($25/month for Standard)

**Backup Verification Checklist:**
[Source: infrastructure/OPERATIONS_RUNBOOK.md Section 5.3]

- [ ] Verify backup completed successfully (check Render dashboard)
- [ ] Check backup file size (should be consistent with database size)
- [ ] Verify backup timestamp (should be within expected schedule)
- [ ] Test restore on staging environment (monthly)
- [ ] Document backup status in operations log

**Manual Backup Procedures:**
[Source: infrastructure/MIGRATION_CHECKLIST.md, infrastructure/OPERATIONS_RUNBOOK.md]

**When to Create Manual Backups:**
- Before production migrations (always)
- Before major schema changes (always)
- Before data anonymization (recommended)
- Before database maintenance operations (recommended)
- Before Render tier upgrades (recommended)

**Manual Backup Methods:**

1. **Using Render CLI:**
   ```bash
   render db backup --database bojin-law-db
   # Verify backup created
   render db backups --database bojin-law-db
   ```

2. **Using pg_dump (PostgreSQL native):**
   ```bash
   # Full database backup
   pg_dump $DATABASE_URL > backup-$(date +%Y%m%d-%H%M%S).sql

   # Compressed backup (recommended)
   pg_dump $DATABASE_URL | gzip > backup-$(date +%Y%m%d-%H%M%S).sql.gz

   # Schema only (no data)
   pg_dump --schema-only $DATABASE_URL > schema-$(date +%Y%m%d).sql

   # Data only (no schema)
   pg_dump --data-only $DATABASE_URL > data-$(date +%Y%m%d).sql
   ```

**Backup Storage:**
[Source: infrastructure/OPERATIONS_RUNBOOK.md]

- **Local Development:** Store in `backups/` directory (gitignored)
- **CI/CD Backups:** Upload to Cloudflare R2 or S3 bucket
- **Retention Policy:** Keep 30 days of manual backups
- **Encryption:** Encrypt backups at rest (AES-256)
- **Access Control:** Limit backup access to DevOps team only

**Database Restore Procedures:**
[Source: infrastructure/MIGRATION_CHECKLIST.md Section on Rollback]

**Restore from Render Backup:**
```bash
# List available backups
render db backups --database bojin-law-db

# Restore specific backup
render db restore --database bojin-law-db-staging --backup [backup-id]

# Wait for restore to complete (5-15 minutes depending on size)
render db status --database bojin-law-db-staging
```

**Restore from pg_dump Backup:**
```bash
# Drop existing database (WARNING: destructive)
dropdb $DATABASE_NAME

# Create fresh database
createdb $DATABASE_NAME

# Restore from backup
psql $DATABASE_URL < backup-20250120-143000.sql

# Or from compressed backup
gunzip -c backup-20250120-143000.sql.gz | psql $DATABASE_URL
```

**Restore Safety Checklist:**
[Source: infrastructure/MIGRATION_CHECKLIST.md]

- [ ] **CRITICAL:** Verify you are restoring to correct environment (staging vs production)
- [ ] Backup current database before restore (backup-before-restore)
- [ ] Stop all application services before restore (prevent write conflicts)
- [ ] Verify backup file integrity (checksum validation)
- [ ] Estimate restore time (communicate downtime to stakeholders)
- [ ] Test database connectivity after restore
- [ ] Run schema validation after restore: `npx prisma migrate status`
- [ ] Run data integrity checks after restore
- [ ] Restart application services after restore
- [ ] Monitor application logs for errors post-restore

**Restore Rollback:**
[Source: infrastructure/OPERATIONS_RUNBOOK.md]

If restore fails or causes issues:
1. Stop application services immediately
2. Restore from pre-restore backup (created in safety checklist)
3. Verify database state matches pre-restore
4. Restart application services
5. Investigate root cause of restore failure
6. Document incident in operations log

### Migration Runbook

**Production Migration Runbook Structure:**
[Source: infrastructure/MIGRATION_CHECKLIST.md, infrastructure/OPERATIONS_RUNBOOK.md]

**Pre-Migration Checklist:**

- [ ] Migration tested on local development database (zero errors)
- [ ] Migration tested on staging environment (zero errors)
- [ ] Full test suite passes on staging post-migration
- [ ] Manual smoke tests completed on staging
- [ ] Migration reviewed by another developer (peer review)
- [ ] Manual backup created: `render db backup --database bojin-law-db`
- [ ] Rollback procedure documented and tested
- [ ] Stakeholders notified 24 hours in advance
- [ ] Maintenance window scheduled (low-traffic period)
- [ ] On-call engineer assigned and available

**Migration Execution Steps:**

1. **Announce maintenance start** (T-0):
   - Post in #engineering Slack channel
   - Update status page (if applicable)
   - Estimated downtime: [X minutes]

2. **Create pre-migration backup** (T+0 to T+2):
   ```bash
   render db backup --database bojin-law-db
   # Verify backup created successfully
   ```

3. **Stop application services** (T+2 to T+5) (if downtime required):
   ```bash
   render scale --service bojin-law-web --replicas 0
   render scale --service bojin-law-gateway --replicas 0
   # Stop other services as needed
   ```

4. **Apply database migration** (T+5 to T+10):
   ```bash
   # Option 1: Via Render Shell
   render shell --service bojin-law-gateway
   npm run db:migrate:deploy

   # Option 2: Directly via psql
   psql $DATABASE_URL -f migrations/003_new_migration.sql
   ```

5. **Verify migration success** (T+10 to T+15):
   ```bash
   # Check migration status
   npx prisma migrate status

   # Verify schema matches expected state
   psql $DATABASE_URL -c "\d users"  # Example: check users table structure

   # Run data integrity checks
   npm run db:validate
   ```

6. **Restart application services** (T+15 to T+20):
   ```bash
   render scale --service bojin-law-web --replicas 2
   render scale --service bojin-law-gateway --replicas 2
   # Restart other services
   ```

7. **Post-migration monitoring** (T+20 to T+60):
   - Check service health endpoints: all services "healthy"
   - Monitor error rates: should be <1%
   - Monitor response times: should be <500ms p95
   - Check database connection pool: <80% utilization
   - Review application logs for migration-related errors

8. **Announce migration complete** (T+60):
   - Post in #engineering Slack channel
   - Update status page
   - Thank team for patience

**Migration Rollback Criteria:**

Rollback if any of these occur:
- Migration fails to apply (SQL error)
- Database schema in inconsistent state
- Application error rate >10% for >5 minutes
- Database connection failures >5% for >5 minutes
- Data integrity violations detected
- Critical functionality broken (user login, case creation, etc.)

**Migration Rollback Procedure:**

1. **Announce rollback decision** (immediate):
   - Post in #engineering Slack: "Initiating migration rollback"
   - Escalate to DevOps lead

2. **Stop application services** (if not already stopped):
   ```bash
   render scale --service bojin-law-web --replicas 0
   render scale --service bojin-law-gateway --replicas 0
   ```

3. **Restore from pre-migration backup**:
   ```bash
   # List backups to find pre-migration backup
   render db backups --database bojin-law-db

   # Restore
   render db restore --database bojin-law-db --backup [backup-id-pre-migration]

   # Wait for restore to complete (5-15 minutes)
   ```

4. **Verify database restored to pre-migration state**:
   ```bash
   npx prisma migrate status
   # Should show migration NOT applied
   ```

5. **Restart application services with previous code version**:
   ```bash
   # Rollback code deployment first
   render rollback --service bojin-law-web --to-deploy [previous-deploy-id]
   render rollback --service bojin-law-gateway --to-deploy [previous-deploy-id]

   # Scale services back up
   render scale --service bojin-law-web --replicas 2
   render scale --service bojin-law-gateway --replicas 2
   ```

6. **Post-rollback verification**:
   - Verify application functionality restored
   - Check error rates returned to normal
   - Monitor for 15 minutes

7. **Post-rollback investigation**:
   - Root cause analysis of migration failure
   - Document lessons learned
   - Fix migration issues before retry

**Migration Communication Template:**
[Source: infrastructure/MIGRATION_CHECKLIST.md]

**24 Hours Before Migration:**
```
Subject: [SCHEDULED MAINTENANCE] Database Migration - [DATE] at [TIME]

Team,

We will be performing a database migration on [DATE] at [TIME] [TIMEZONE].

Purpose: [Brief description of migration]
Estimated Downtime: [X minutes] or [Zero downtime - blue-green deployment]
Affected Features: [List any features that may be temporarily unavailable]

Timeline:
- [TIME]: Maintenance window begins
- [TIME]: Expected completion
- [TIME]: Rollback deadline (if issues detected)

In case of issues, contact:
- On-call engineer: [NAME] via [PHONE/SLACK]
- DevOps lead: [NAME] via [PHONE/SLACK]

Rollback Criteria:
- Error rate >10% for >5 minutes
- Critical functionality broken
- Data integrity issues detected

Thank you for your patience.
```

### Zero-Downtime Migration Strategy

**Expand-Contract Pattern:**
[Source: infrastructure/MIGRATION_CHECKLIST.md, Database best practices]

The expand-contract pattern enables zero-downtime migrations for breaking schema changes by splitting the migration into multiple backward-compatible steps.

**Example: Renaming a Column**

Old schema: `users` table has `name` column
New schema: `users` table should have `full_name` column

**Step 1: Expand (Add new column)** - Zero downtime
```sql
-- Migration 003_add_full_name_column.sql
ALTER TABLE users ADD COLUMN full_name VARCHAR(200);
```
- Deploy this migration
- Old code continues using `name` column (still exists)
- New code not deployed yet

**Step 2: Dual-Write (Update application code)** - Zero downtime
```typescript
// Update Prisma schema
model User {
  name      String?     // Mark as optional
  full_name String?     // Mark as optional
}

// Update application code to write to both columns
await prisma.user.create({
  data: {
    name: fullName,      // Write to old column
    full_name: fullName  // Write to new column
  }
});
```
- Deploy application code update
- All new writes go to both columns
- Old code can still read from `name` column

**Step 3: Backfill Data** - Zero downtime
```sql
-- Migration 004_backfill_full_name.sql
UPDATE users SET full_name = name WHERE full_name IS NULL;
```
- Run as background job (low priority to avoid locking)
- Verify all rows backfilled: `SELECT COUNT(*) FROM users WHERE full_name IS NULL;`

**Step 4: Switch Reads (Update application code)** - Zero downtime
```typescript
// Update application code to read from new column
const user = await prisma.user.findUnique({
  where: { id: userId }
});
console.log(user.full_name); // Use full_name instead of name
```
- Deploy application code update
- All reads now use `full_name` column
- Old code no longer used in production

**Step 5: Contract (Remove old column)** - Zero downtime
```sql
-- Migration 005_remove_name_column.sql
ALTER TABLE users DROP COLUMN name;
```
- Deploy this migration
- Only after Step 4 deployed and verified stable for 24+ hours

**Step 6: Cleanup Prisma Schema** - Zero downtime
```typescript
// Final Prisma schema
model User {
  full_name String  // Required field, single source of truth
}
```
- Deploy application code update
- Migration complete

**Backward-Compatible Migration Strategies:**
[Source: Database migration best practices]

**Additive Changes (Always zero-downtime):**
- Adding new tables
- Adding new columns (nullable or with defaults)
- Adding new indexes (use CONCURRENTLY in PostgreSQL)
- Adding new constraints (as NOT VALID, then validated later)

**Destructive Changes (Require expand-contract):**
- Renaming columns (use expand-contract as shown above)
- Renaming tables (use views as aliases during transition)
- Changing column types (add new column, migrate data, drop old)
- Removing columns (mark as deprecated, stop using, then remove)
- Removing tables (mark as deprecated, stop using, then remove)

**Index Creation Without Locking:**
[Source: PostgreSQL best practices]

```sql
-- Bad: Locks table during index creation (downtime)
CREATE INDEX idx_users_email ON users(email);

-- Good: Creates index without locking table (zero downtime)
CREATE INDEX CONCURRENTLY idx_users_email ON users(email);
```

**Migration Decision Matrix:**
[Source: infrastructure/MIGRATION_CHECKLIST.md]

| Change Type              | Backward Compatible? | Downtime Required? | Strategy                |
|--------------------------|----------------------|--------------------|-------------------------|
| Add table                | Yes                  | No                 | Direct migration        |
| Add column (nullable)    | Yes                  | No                 | Direct migration        |
| Add column (required)    | No                   | No                 | Add nullable → backfill → make required |
| Rename column            | No                   | No                 | Expand-contract pattern |
| Remove column            | No                   | No                 | Deprecate → stop using → remove |
| Change column type       | No                   | No                 | Add new → migrate → remove old |
| Add index                | Yes                  | No                 | CREATE INDEX CONCURRENTLY |
| Add foreign key          | No (can block)       | Maybe              | Add as NOT VALID → validate separately |
| Large data backfill      | Depends              | No                 | Batch processing with throttling |

**Feature Flags for Schema Changes:**
[Source: Best practices]

Use feature flags to gradually roll out schema changes:

```typescript
// Feature flag configuration
const useNewSchema = getFeatureFlag('use_full_name_column', {
  default: false,
  rollout: 'gradual'  // 10% → 50% → 100%
});

// Conditional logic based on feature flag
if (useNewSchema) {
  console.log(user.full_name);
} else {
  console.log(user.name);
}
```

**Blue-Green Deployment for Breaking Changes:**
[Source: infrastructure/DEPLOYMENT_GUIDE.md]

For complex migrations that cannot use expand-contract:

1. **Prepare Blue Environment (current production)**
   - Running with old schema

2. **Prepare Green Environment (new version)**
   - Clone database to green environment
   - Apply migrations to green database
   - Deploy new application code to green environment
   - Test green environment thoroughly

3. **Switch Traffic to Green**
   - Update load balancer to point to green environment
   - Monitor for issues (5-10 minutes)
   - Keep blue environment running for quick rollback

4. **Decommission Blue** (after 24-48 hours stability)
   - Stop blue environment services
   - Archive blue database

### File Locations

**Database Package Structure:**
[Source: architecture/unified-project-structure.md, packages/database/README.md]

```
packages/database/
├── prisma/
│   ├── schema.prisma              # Prisma schema definition
│   ├── seed.ts                    # Seed data script (NEW in this story)
│   └── migrations/                # Prisma-generated migrations
│       ├── 20250120_init/
│       ├── 20250121_add_users/
│       └── migration_lock.toml
├── scripts/                        # Database utility scripts (NEW in this story)
│   ├── run-migration.sh           # Migration execution wrapper
│   ├── rollback-migration.sh      # Migration rollback utility
│   ├── migration-history.sh       # View migration history
│   ├── anonymize-data.ts          # Data anonymization script
│   ├── export-production.sh       # Production data export
│   ├── import-anonymized.sh       # Import and anonymize workflow
│   ├── backup-database.sh         # Manual backup script
│   └── restore-database.sh        # Database restore script
├── src/
│   ├── client.ts                  # Prisma Client singleton (from Story 2.2)
│   ├── redis.ts                   # Redis client (from Story 2.2)
│   └── index.ts                   # Package exports
├── package.json                   # NPM scripts for migrations, seed, backup
├── tsconfig.json
└── README.md                      # Usage documentation
```

**Migration Scripts npm Commands:**
[Source: package.json]

Add to `packages/database/package.json`:
```json
{
  "scripts": {
    "db:migrate": "prisma migrate dev",
    "db:migrate:deploy": "prisma migrate deploy",
    "db:migrate:status": "prisma migrate status",
    "db:migrate:undo": "./scripts/rollback-migration.sh",
    "db:migrate:history": "./scripts/migration-history.sh",
    "db:seed": "prisma db seed",
    "db:anonymize": "ts-node scripts/anonymize-data.ts",
    "db:export": "./scripts/export-production.sh",
    "db:import:anonymized": "./scripts/import-anonymized.sh",
    "db:backup": "./scripts/backup-database.sh",
    "db:restore": "./scripts/restore-database.sh",
    "db:validate": "ts-node scripts/validate-data-integrity.ts"
  }
}
```

**Runbook and Documentation Locations:**
[Source: architecture/unified-project-structure.md]

```
docs/
├── runbooks/                       # Operational runbooks (NEW in this story)
│   ├── database-migration-runbook.md    # Production migration procedures
│   ├── database-quick-start.md          # Developer quick start guide
│   └── migration-risk-assessment.md     # Migration risk checklist
├── templates/                      # Document templates (NEW in this story)
│   └── migration-announcement-template.md  # Communication template
└── architecture/
    └── database-migration-patterns.md    # Zero-downtime patterns (NEW in this story)

infrastructure/
├── OPERATIONS_RUNBOOK.md          # Updated with backup/restore procedures
└── MIGRATION_CHECKLIST.md         # Already exists from Story 2.1.1
```

**Integration Test Location:**
[Source: architecture/testing-strategy.md]

```
tests/
└── integration/
    └── migrations.test.ts          # Migration integration tests (NEW in this story)
```

### Testing Requirements

**Test File Locations:**
[Source: architecture/testing-strategy.md]

- Unit tests: `packages/database/scripts/__tests__/` (for script utilities)
- Integration tests: `tests/integration/migrations.test.ts`
- E2E tests: Not required for this story (database infrastructure)

**Testing Standards:**
[Source: architecture/testing-strategy.md, architecture/coding-standards.md]

1. **Unit Tests (30% of test effort):**
   - Test migration script utilities (parsing, validation)
   - Test anonymization logic (field replacement)
   - Test seed data generation functions
   - Coverage target: 80%

2. **Integration Tests (70% of test effort - primary focus):**
   - Test: Create Prisma migration from schema change
   - Test: Apply migration to test database
   - Test: Rollback migration successfully
   - Test: Migration history tracking accurate
   - Test: Seed data script execution (idempotency)
   - Test: Data anonymization correctness (no PII remains)
   - Test: Backup and restore integrity (data matches)
   - Test: Zero-downtime migration pattern (expand-contract)

3. **Testing Framework:**
   - **Framework:** Jest 29+
   - **Database Testing:** Create test database per test suite
   - **Cleanup:** Use `afterAll()` to drop test database
   - **Assertions:** Expect API (built-in Jest)
   - **Database Queries:** Direct SQL via `prisma.$queryRaw` for verification

4. **Test Patterns:**
   - Use `beforeAll()` to create test database
   - Use `beforeEach()` to reset database to clean state
   - Use `afterEach()` to cleanup test data
   - Use `afterAll()` to drop test database
   - Test both success and failure scenarios
   - Include timing tests (migration should complete in <30s for test schema)

**Example Integration Test Structure:**

```typescript
// tests/integration/migrations.test.ts
import { PrismaClient } from '@prisma/client';
import { execSync } from 'child_process';

describe('Database Migration Integration Tests', () => {
  let prisma: PrismaClient;
  const TEST_DATABASE_URL = process.env.TEST_DATABASE_URL;

  beforeAll(async () => {
    // Create test database
    execSync(`createdb test_migrations_${Date.now()}`);
    prisma = new PrismaClient({ datasources: { db: { url: TEST_DATABASE_URL } } });
  });

  afterAll(async () => {
    await prisma.$disconnect();
    // Drop test database
    execSync(`dropdb test_migrations_${Date.now()}`);
  });

  describe('Prisma Migrations', () => {
    it('should create migration from schema change', () => {
      // Modify schema.prisma
      // Run: npx prisma migrate dev --name test_migration
      // Verify migration file created in prisma/migrations/
    });

    it('should apply migration successfully', async () => {
      execSync('npx prisma migrate deploy', { env: { DATABASE_URL: TEST_DATABASE_URL } });

      // Verify migration applied
      const migrations = await prisma.$queryRaw`
        SELECT * FROM _prisma_migrations WHERE migration_name = 'test_migration'
      `;
      expect(migrations).toHaveLength(1);
    });

    it('should rollback migration successfully', async () => {
      // Run rollback script
      execSync('./packages/database/scripts/rollback-migration.sh test_migration');

      // Verify migration rolled back
      const migrations = await prisma.$queryRaw`
        SELECT * FROM _prisma_migrations WHERE migration_name = 'test_migration' AND rolled_back_at IS NOT NULL
      `;
      expect(migrations).toHaveLength(1);
    });
  });

  describe('Seed Data', () => {
    it('should seed database with test data', async () => {
      execSync('npm run db:seed', { cwd: 'packages/database' });

      const firms = await prisma.firm.count();
      const users = await prisma.user.count();
      const cases = await prisma.case.count();

      expect(firms).toBe(1);
      expect(users).toBe(5); // 1 Partner + 2 Associates + 2 Paralegals
      expect(cases).toBe(10);
    });

    it('should be idempotent (no duplicates on re-run)', async () => {
      execSync('npm run db:seed', { cwd: 'packages/database' });
      execSync('npm run db:seed', { cwd: 'packages/database' }); // Run twice

      const firms = await prisma.firm.count();
      expect(firms).toBe(1); // Still only 1 firm
    });
  });

  describe('Data Anonymization', () => {
    it('should anonymize PII fields correctly', async () => {
      // Seed database with real-looking data
      await prisma.user.create({
        data: {
          email: 'john.doe@realfirm.ro',
          first_name: 'John',
          last_name: 'Doe'
        }
      });

      // Run anonymization
      execSync('npm run db:anonymize', { cwd: 'packages/database' });

      // Verify anonymization
      const users = await prisma.user.findMany();
      expect(users[0].email).toMatch(/demo\d+@example\.com/);
      expect(users[0].first_name).toMatch(/Demo User \d+/);
      expect(users[0].email).not.toContain('john.doe');
    });
  });

  describe('Backup and Restore', () => {
    it('should backup and restore database successfully', async () => {
      // Insert test data
      const testUser = await prisma.user.create({
        data: { email: 'test@example.com', first_name: 'Test', last_name: 'User' }
      });

      // Backup database
      execSync('npm run db:backup', { cwd: 'packages/database' });

      // Delete test data
      await prisma.user.delete({ where: { id: testUser.id } });

      // Restore database
      execSync('npm run db:restore', { cwd: 'packages/database' });

      // Verify data restored
      const restoredUser = await prisma.user.findUnique({ where: { id: testUser.id } });
      expect(restoredUser).toBeDefined();
      expect(restoredUser.email).toBe('test@example.com');
    });
  });
});
```

### Technical Constraints

**Performance Requirements:**
[Source: infrastructure/OPERATIONS_RUNBOOK.md]

- Migration execution time: <5 minutes for typical schema changes
- Seed data script execution: <30 seconds for full seed
- Anonymization script: <2 minutes for development database (~1000 records)
- Backup creation: <5 minutes for 25GB database
- Restore operation: <15 minutes for 25GB database

**Prisma Constraints:**
[Source: Prisma documentation, architecture/tech-stack.md]

- Prisma Migrate does not support automatic rollback (design decision)
- Migration files are immutable (cannot edit after applied)
- Shadow database required for dev migrations (Render provides automatically)
- Extensions must be enabled before Prisma Migrate can use them
- Large data migrations should be split into batches (avoid locking)

**PostgreSQL Constraints:**
[Source: PostgreSQL documentation]

- Index creation locks table unless using CONCURRENTLY
- ALTER TABLE can lock table (use ADD COLUMN with DEFAULT carefully)
- Foreign key validation can be slow on large tables (add as NOT VALID first)
- Large backfills should be batched to avoid transaction timeout

**Render Database Constraints:**
[Source: infrastructure/README.md, Story 2.2]

- Max connections: 20 (Standard tier)
- Backup retention: 7 days (Standard tier)
- Backup schedule: Fixed at 2:00 AM UTC (cannot customize)
- Restore time: Depends on database size (5-15 minutes typical)
- No direct file system access to backup files (use Render CLI)

**Security Requirements:**
[Source: architecture/coding-standards.md]

- Never commit seed data with real PII to git
- Encrypt backup files at rest (AES-256)
- Limit access to production backups (DevOps team only)
- Anonymize production data before importing to development
- Rotate database credentials every 90 days
- Log all database restore operations (audit trail)

### Data Integrity Considerations

**Referential Integrity:**
[Source: architecture/database-schema.md]

- All foreign keys must be valid UUIDs referencing existing records
- Seed data must create entities in correct order (parents before children)
- Cascading deletes configured appropriately (e.g., case deleted → documents deleted)
- Orphaned records should be prevented by database constraints

**Seed Data Relationships:**
```
Law Firm (1)
  └── Users (5)
       └── Cases (10)
            ├── Documents (20)
            └── Tasks (30)
```

**Migration Safety:**
- Always test migrations on staging before production
- Verify foreign key constraints not violated after migration
- Run data validation script post-migration
- Check for NULL values in required fields
- Verify enum values still valid after migration

**Backup Validation:**
- Verify backup file size consistent with database size
- Test restore on staging environment monthly
- Validate data integrity post-restore (row counts match)
- Check for corruption (pg_dump exit code 0)

## Change Log

| Date       | Version | Description                   | Author                  |
|------------|---------|-------------------------------|-------------------------|
| 2025-11-20 | 1.0     | Initial story creation with comprehensive migration, seeding, and backup infrastructure context | Bob (Scrum Master) |

## Dev Agent Record

### Agent Model Used

_To be populated by Dev Agent during implementation_

### Debug Log References

_To be populated by Dev Agent during implementation_

### Completion Notes

_To be populated by Dev Agent during implementation_

### File List

_To be populated by Dev Agent during implementation_

## QA Results

[To be populated after QA review]
