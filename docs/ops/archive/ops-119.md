# OPS-119: AI Assistant Context Integration

## Summary

Integrate the User Daily Context and Case Briefing services into the AI assistant, injecting pre-computed context into the system prompt for faster, higher-quality responses.

## Problem

Currently the AI assistant's system prompt contains:

- Current date and user name (~50 tokens)
- Case name if in case context (~20 tokens)
- Tool usage instructions (~500 tokens)
- **Total context about user's state: ~0 tokens**

Claude must discover everything through tool calls, adding 2-4s latency for common questions.

## Solution

Inject pre-computed context from OPS-117 and OPS-118:

- User Daily Context (~600-800 tokens)
- Case Briefing when in case context (~400-600 tokens)

**Total prompt increase**: ~1000-1400 tokens
**Expected latency reduction**: 2-3s for common queries

## Implementation

### Updated System Prompt Structure

```typescript
// services/gateway/src/services/ai-system-prompt.ts

export async function buildSystemPrompt(context: AssistantContext): Promise<string> {
  // Base prompt (existing)
  let prompt = BASE_SYSTEM_PROMPT;

  // Current date and user (existing)
  prompt += `\n\n## Informații sesiune\n`;
  prompt += `Data: ${formatDate(new Date())}\n`;
  prompt += `Utilizator: ${context.userName} (${context.userRole})\n`;

  // NEW: User daily context
  const userContext = await userContextService.getContextForPrompt(context.userId, context.firmId);
  if (userContext) {
    prompt += `\n## Starea ta curentă\n\n${userContext}\n`;
  }

  // NEW: Case briefing (if in case context)
  if (context.caseId) {
    const caseBriefing = await caseBriefingService.getBriefingText(context.caseId);
    prompt += `\n## Dosarul curent\n\n${caseBriefing}\n`;
  }

  // Tool instructions (existing)
  prompt += `\n## Instrucțiuni pentru tools\n\n${TOOL_INSTRUCTIONS}`;

  return prompt;
}
```

### Updated Morning Briefing Flow

Currently, `get_morning_briefing` tool makes multiple DB queries. With context:

```typescript
// In ai-assistant.service.ts

case 'get_morning_briefing': {
  // Instead of querying everything, use pre-computed context
  const context = await userContextService.getContext(userId, firmId);

  return {
    greeting: this.generateGreeting(context),
    todayEvents: context.todayEvents,
    urgentItems: context.urgentItems,
    newItems: {
      emails: context.newEmailsCount,
      documents: context.newDocumentsCount,
    },
    activeCases: context.activeCases,
  };
}
```

### Latency Comparison

| Query                                 | Before (tool calls) | After (with context) |
| ------------------------------------- | ------------------- | -------------------- |
| "Ce am de făcut azi?"                 | 3 tools, ~3s        | 0 tools, ~1s         |
| "Când e următorul termen la Solaria?" | 2 tools, ~2s        | 0 tools, ~1s         |
| "Ce emailuri noi am?"                 | 1 tool, ~1s         | 0 tools, ~1s         |
| "Rezumă dosarul curent"               | 1 tool, ~1.5s       | 0 tools, ~1s         |

### Handling "What's New" Queries

With context, Claude can directly answer:

- "Ce s-a întâmplat de ieri?" → Uses `recentActivity` from context
- "Am emailuri noi?" → Uses `newEmailsCount` from context
- "Ce urgențe am?" → Uses `urgentItems` from context

### Cache Warming Strategy

To avoid cold-start latency:

```typescript
// Warm cache on user login
async function onUserLogin(userId: string, firmId: string): Promise<void> {
  // Fire and forget - don't block login
  userContextService.getContext(userId, firmId).catch(console.error);
}

// Warm case briefing when user navigates to case
async function onCaseView(caseId: string): Promise<void> {
  caseBriefingService.getBriefing(caseId).catch(console.error);
}
```

### Token Budget Monitoring

Add logging to track actual token usage:

```typescript
const systemPrompt = await buildSystemPrompt(context);
const systemPromptTokens = this.estimateTokens(systemPrompt);

logger.info('AI request', {
  userId: context.userId,
  systemPromptTokens,
  hasUserContext: !!userContext,
  hasCaseBriefing: !!context.caseId,
});

// Alert if exceeding budget
if (systemPromptTokens > 2000) {
  logger.warn('System prompt exceeds token budget', { systemPromptTokens });
}
```

## Files to Create/Modify

### Modified Files

- `services/gateway/src/services/ai-system-prompt.ts` - Inject contexts
- `services/gateway/src/services/ai-assistant.service.ts` - Use contexts, update tools
- `services/gateway/src/graphql/resolvers/ai-assistant.resolvers.ts` - Warm cache on login

## Dependencies

- OPS-115: AI Context Files - Data Model
- OPS-117: User Daily Context Service
- OPS-118: Case Briefing Service

## Enables

- OPS-120: Notification Engine (shares context infrastructure)

## Success Metrics

- [ ] Average response latency reduced by 40%+
- [ ] Tool calls per query reduced by 50%+
- [ ] Morning briefing loads in <1.5s
- [ ] System prompt stays under 2500 tokens total
- [ ] No regression in response quality

## Testing

1. **Unit tests**
   - System prompt generation includes contexts
   - Token counting is accurate

2. **Integration tests**
   - Morning briefing uses cached context
   - Case queries use briefing
   - Context invalidation works

3. **Load testing**
   - Context generation under load
   - Cache performance

## Rollout Plan

1. **Phase 1**: Add context injection behind feature flag
2. **Phase 2**: A/B test with 10% of users
3. **Phase 3**: Measure latency and quality metrics
4. **Phase 4**: Full rollout if metrics positive

---

## Local Verification Status

| Step           | Status     | Date | Notes |
| -------------- | ---------- | ---- | ----- |
| Prod data test | ⬜ Pending |      |       |
| Preflight      | ⬜ Pending |      |       |
| Docker test    | ⬜ Pending |      |       |

**Verified**: No

---

## Files Involved

- `services/gateway/src/services/ai-system-prompt.ts` (inject contexts)
- `services/gateway/src/services/ai-assistant.service.ts` (use contexts)
- `services/gateway/src/services/user-context.service.ts` (OPS-117 dependency)
- `services/gateway/src/services/case-briefing.service.ts` (OPS-118 dependency)

---

## Session Log

### 2024-12-23 - Session 1 - Implementation Complete

**Work Done:**

1. Updated `SystemPromptContext` interface to include `caseBriefing?: string`
2. Added `{caseBriefing}` placeholder to system prompt template (after userDailyContext)
3. Updated `buildSystemPrompt()` to replace the caseBriefing placeholder
4. Integrated `caseBriefingService.getBriefingText()` in `buildContextualSystemPrompt()`:
   - Only called when `caseId` is present (user is on a case page)
   - Non-blocking - errors are logged but don't fail the request

**Integration Summary:**

- User context (OPS-117): ✅ Already integrated, working
- Case briefing (OPS-118): ✅ Now integrated

**System Prompt Flow:**

```
AI Request (with context)
    │
    ├── Load user daily context (OPS-117) ~600-800 tokens
    │
    ├── If caseId present:
    │   └── Load case briefing (OPS-118) ~400-600 tokens
    │
    └── Build final prompt with both contexts
```

**Preflight Status:**

- ✅ TypeScript compiles
- ✅ Production build succeeds
- ✅ Docker builds (web + gateway)
- ⚠️ ESLint warnings (non-blocking, pre-existing)
- ⚠️ One unit test failure (pre-existing, unrelated to OPS-119)

**Status**: Implementation complete, ready for local verification

### 2024-12-23 - Issue Created

- Created from `/ops-ideate` session exploring AI context optimization
- Part of AI Context Files epic (OPS-115 → OPS-120)
